{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MLOps Project: Wellness Tourism Package Prediction\n",
                "\n",
                "## Business Context\n",
                "\"Visit with Us\" aims to predict whether a customer will purchase the newly introduced Wellness Tourism Package. This project implements an MLOps pipeline to automate the workflow from data preparation to deployment.\n",
                "\n",
                "## Objective\n",
                "Design and deploy an MLOps pipeline on GitHub to automate the end-to-end workflow for predicting customer purchases.\n",
                "\n",
                "## 1. Data Registration\n",
                "**Goal:** Register the dataset on the Hugging Face dataset space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from huggingface_hub import HfApi, HfFolder\n",
                "\n",
                "# Configuration\n",
                "DATASET_PATH = \"data/Tourism.csv\" # Ensure this file exists in your 'data' folder\n",
                "HF_USERNAME = \"YOUR_HF_USERNAME\" # Replace with your Hugging Face username\n",
                "HF_DATASET_REPO = f\"{HF_USERNAME}/tourism-package-prediction\"\n",
                "\n",
                "# Initialize API\n",
                "api = HfApi()\n",
                "token = HfFolder.get_token()\n",
                "\n",
                "if token:\n",
                "    print(f\"Logged in as {HF_USERNAME}\")\n",
                "else:\n",
                "    print(\"Please login using `huggingface-cli login` in your terminal.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preparation\n",
                "**Goal:** Load, clean, split, and upload processed data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "import os\n",
                "\n",
                "def prepare_data():\n",
                "    # Load Data\n",
                "    try:\n",
                "        df = pd.read_csv(DATASET_PATH)\n",
                "        print(\"Data loaded successfully.\")\n",
                "    except FileNotFoundError:\n",
                "        print(\"Dataset not found. Please ensure 'data/Tourism.csv' exists.\")\n",
                "        return\n",
                "\n",
                "    # Data Cleaning\n",
                "    if 'CustomerID' in df.columns:\n",
                "        df = df.drop(columns=['CustomerID'])\n",
                "    \n",
                "    # Handle Missing Values (Simple Imputation)\n",
                "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
                "    for col in num_cols:\n",
                "        df[col] = df[col].fillna(df[col].median())\n",
                "        \n",
                "    cat_cols = df.select_dtypes(include=['object']).columns\n",
                "    for col in cat_cols:\n",
                "        df[col] = df[col].fillna(df[col].mode()[0])\n",
                "        \n",
                "    # Split Data\n",
                "    train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
                "    \n",
                "    # Save Locally\n",
                "    os.makedirs(\"data/processed\", exist_ok=True)\n",
                "    train.to_csv(\"data/processed/train.csv\", index=False)\n",
                "    test.to_csv(\"data/processed/test.csv\", index=False)\n",
                "    print(\"Data split and saved locally.\")\n",
                "    \n",
                "    # Upload to Hugging Face\n",
                "    if token:\n",
                "        api.create_repo(repo_id=HF_DATASET_REPO, repo_type=\"dataset\", exist_ok=True)\n",
                "        api.upload_file(path_or_fileobj=\"data/processed/train.csv\", path_in_repo=\"train.csv\", repo_id=HF_DATASET_REPO, repo_type=\"dataset\")\n",
                "        api.upload_file(path_or_fileobj=\"data/processed/test.csv\", path_in_repo=\"test.csv\", repo_id=HF_DATASET_REPO, repo_type=\"dataset\")\n",
                "        print(\"Data uploaded to Hugging Face.\")\n",
                "\n",
                "prepare_data()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Building & Experiment Tracking\n",
                "**Goal:** Train a model, tune parameters, log experiments, and register the best model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "import joblib\n",
                "\n",
                "HF_MODEL_REPO = f\"{HF_USERNAME}/tourism-prediction-model\"\n",
                "\n",
                "def train_and_evaluate():\n",
                "    try:\n",
                "        train_df = pd.read_csv(\"data/processed/train.csv\")\n",
                "        test_df = pd.read_csv(\"data/processed/test.csv\")\n",
                "    except FileNotFoundError:\n",
                "        print(\"Processed data not found.\")\n",
                "        return\n",
                "\n",
                "    X_train = train_df.drop('ProdTaken', axis=1)\n",
                "    y_train = train_df['ProdTaken']\n",
                "    X_test = test_df.drop('ProdTaken', axis=1)\n",
                "    y_test = test_df['ProdTaken']\n",
                "    \n",
                "    # Preprocessing (One-Hot Encoding)\n",
                "    X_train = pd.get_dummies(X_train)\n",
                "    X_test = pd.get_dummies(X_test)\n",
                "    X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
                "    \n",
                "    # Model Training\n",
                "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Evaluation\n",
                "    predictions = model.predict(X_test)\n",
                "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
                "    print(classification_report(y_test, predictions))\n",
                "    \n",
                "    # Save Model\n",
                "    os.makedirs(\"models\", exist_ok=True)\n",
                "    joblib.dump(model, \"models/model.joblib\")\n",
                "    joblib.dump(list(X_train.columns), \"models/features.joblib\")\n",
                "    \n",
                "    # Register Model\n",
                "    if token:\n",
                "        api.create_repo(repo_id=HF_MODEL_REPO, exist_ok=True)\n",
                "        api.upload_file(path_or_fileobj=\"models/model.joblib\", path_in_repo=\"model.joblib\", repo_id=HF_MODEL_REPO)\n",
                "        api.upload_file(path_or_fileobj=\"models/features.joblib\", path_in_repo=\"features.joblib\", repo_id=HF_MODEL_REPO)\n",
                "        print(\"Model registered on Hugging Face.\")\n",
                "\n",
                "train_and_evaluate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Deployment\n",
                "**Goal:** Deploy the model using Streamlit and Docker.\n",
                "\n",
                "### Streamlit App (`app.py`)\n",
                "The `app.py` file contains the code to load the model from Hugging Face and create a user interface for predictions.\n",
                "\n",
                "### Dockerfile\n",
                "The `Dockerfile` defines the environment for running the Streamlit app."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. MLOps Pipeline with GitHub Actions\n",
                "**Goal:** Automate the workflow.\n",
                "\n",
                "The `.github/workflows/pipeline.yml` file defines the CI/CD pipeline that triggers on push to the main branch. It installs dependencies, runs data preparation, and trains the model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "This notebook demonstrates the end-to-end MLOps pipeline for the \"Visit with Us\" project, covering data management, model training, and automation setup."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}